{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f73eb0b-c1b7-4d62-bec3-5dc5d2b3e233",
      "metadata": {
        "id": "3f73eb0b-c1b7-4d62-bec3-5dc5d2b3e233",
        "outputId": "f3a24b20-2c9b-4b2e-b067-9e79abab5fa2",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/jupyter'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d1a6a2f-edc3-498a-8fff-3a8c4a9a0294",
      "metadata": {
        "id": "7d1a6a2f-edc3-498a-8fff-3a8c4a9a0294",
        "tags": []
      },
      "outputs": [],
      "source": [
        "os.chdir('/home/jupyter')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc12900-b3e5-41fe-b9e3-8f52c862dcc3",
      "metadata": {
        "id": "2fc12900-b3e5-41fe-b9e3-8f52c862dcc3",
        "outputId": "31aba2aa-4053-4518-ffe0-64da670bb5bb",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Apr 26 19:25:06 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   29C    P0              57W / 400W |  21323MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:00:05.0 Off |                    0 |\n",
            "| N/A   31C    P0              59W / 400W |  26665MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:00:06.0 Off |                    0 |\n",
            "| N/A   28C    P0              60W / 400W |  26665MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:00:07.0 Off |                    0 |\n",
            "| N/A   30C    P0              59W / 400W |  18801MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A     22737      C   /opt/conda/envs/llava1.5env/bin/python    21310MiB |\n",
            "|    1   N/A  N/A     22737      C   /opt/conda/envs/llava1.5env/bin/python    26652MiB |\n",
            "|    2   N/A  N/A     22737      C   /opt/conda/envs/llava1.5env/bin/python    26652MiB |\n",
            "|    3   N/A  N/A     22737      C   /opt/conda/envs/llava1.5env/bin/python    18788MiB |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b057187-f300-4821-800e-79bd847b6ee6",
      "metadata": {
        "id": "6b057187-f300-4821-800e-79bd847b6ee6",
        "outputId": "16a555c1-76e2-4196-eb98-28d174e483e7",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch Version: 2.1.2+cu121\n",
            "CUDA Available: True\n",
            "CUDA Version: 12.1\n",
            "CUDA Device Name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"PyTorch Version:\", torch.__version__)\n",
        "print(\"CUDA Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA Version:\", torch.version.cuda)\n",
        "    print(\"CUDA Device Name:\", torch.cuda.get_device_name(0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df6cef5b-9a0c-46a8-9a9f-f3e6fd715674",
      "metadata": {
        "id": "df6cef5b-9a0c-46a8-9a9f-f3e6fd715674",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Install or upgrade jupyter and ipywidgets using the current Python executable\n",
        "!pip install --upgrade jupyter ipywidgets\n",
        "\n",
        "# Enable the widgetsnbextension using the current Python executable\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c6faa1a-01a3-4c64-98c3-cdc21dd0ee6a",
      "metadata": {
        "id": "0c6faa1a-01a3-4c64-98c3-cdc21dd0ee6a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip\n",
        "!pip install matplotlib -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ebbca83-338c-4fda-aad0-65a89c88ab21",
      "metadata": {
        "id": "2ebbca83-338c-4fda-aad0-65a89c88ab21",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install  datasets\n",
        "# !pip install -q git+https://github.com/huggingface/transformers.git@add-model-idefics\n",
        "!pip install transformers==4.37.2  -U\n",
        "!pip install  bitsandbytes sentencepiece==0.1.99 accelerate==0.21.0 loralib\n",
        "!pip install  peft\n",
        "!pip install hf_transfer  -U\n",
        "!pip install pickleshare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70fc8fe2-d30f-4acf-ae21-015fc95c9750",
      "metadata": {
        "id": "70fc8fe2-d30f-4acf-ae21-015fc95c9750",
        "outputId": "0c25d3c5-22c0-41da-df81-56b0cd0b40fa",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLaVA directory already exists. Skipping clone.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir(\"LLaVA\"):\n",
        "    !git clone https://github.com/haotian-liu/LLaVA.git\n",
        "else:\n",
        "    print(\"LLaVA directory already exists. Skipping clone.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f3a780-6748-4971-b736-e9746810467d",
      "metadata": {
        "id": "e0f3a780-6748-4971-b736-e9746810467d",
        "outputId": "7fde0f66-5650-4bab-b09f-696f7b10c1ba",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The script has been updated successfully.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Define the path to the builder.py file\n",
        "file_path = 'LLaVA/llava/model/builder.py'\n",
        "\n",
        "# Read the content of the file\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Regular expression to find the block between 'vision_tower = model.get_vision_tower()' and 'image_processor = vision_tower.image_processor'\n",
        "pattern_block = (\n",
        "    r'(vision_tower = model.get_vision_tower\\(\\)\\n)'\n",
        "    r'.*?'  # Non-greedy match for any characters (including newlines) between the two markers\n",
        "    r'(image_processor = vision_tower.image_processor)'\n",
        ")\n",
        "\n",
        "replacement_block = (\n",
        "    r'\\1'  # Keep the starting line unchanged\n",
        "    '        if not vision_tower.is_loaded:\\n'\n",
        "    '            print(\\'vision_tower is not loaded so loading it now\\')\\n'\n",
        "    '            vision_tower.load_model(device_map=device_map)\\n'\n",
        "    '            vision_tower.to(device=device, dtype=torch.bfloat16)\\n'\n",
        "    '        else:\\n'\n",
        "    '            print(\\'vision_tower is loaded\\')\\n'\n",
        "    r'        \\2'  # Keep the ending line unchanged\n",
        ")\n",
        "\n",
        "# Replace the specific block\n",
        "content = re.sub(pattern_block, replacement_block, content, flags=re.DOTALL)\n",
        "\n",
        "# Write the modified content back to the file\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(content)\n",
        "\n",
        "print('The script has been updated successfully.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5300b5b1-eb4e-4e8c-85f5-19effd2a1d46",
      "metadata": {
        "id": "5300b5b1-eb4e-4e8c-85f5-19effd2a1d46",
        "outputId": "56e32318-0c9f-41ba-cfc8-22819ada3e15",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No replacement needed. All instances of float16 already have 'b' before them.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Define the path to the builder.py file\n",
        "file_path = 'LLaVA/llava/model/builder.py'\n",
        "\n",
        "# Read the content of the file\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Regular expression to find 'float16' not preceded by 'b'\n",
        "pattern = r'(?<!b)float16'\n",
        "\n",
        "# Check if there are any matches\n",
        "if re.search(pattern, content):\n",
        "    # Replace 'float16' with 'bfloat16'\n",
        "    modified_content = re.sub(pattern, 'bfloat16', content)\n",
        "\n",
        "    # Write the modified content back to the file\n",
        "    with open(file_path, 'w') as file:\n",
        "        file.write(modified_content)\n",
        "\n",
        "    print('All necessary instances of float16 have been replaced with bfloat16 in', file_path)\n",
        "else:\n",
        "    print('No replacement needed. All instances of float16 already have \\'b\\' before them.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35255a94-5b44-42d0-9994-3bd573c3098a",
      "metadata": {
        "id": "35255a94-5b44-42d0-9994-3bd573c3098a",
        "outputId": "9044dcc3-a69b-415f-b42c-80dbed8ee4e3",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/jupyter'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17dfd080-b91a-4f1c-9eea-0a8e87c283bc",
      "metadata": {
        "id": "17dfd080-b91a-4f1c-9eea-0a8e87c283bc",
        "outputId": "a5c34190-3334-4e40-b729-d4c91c1f2cfa",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/jupyter/LLaVA\n"
          ]
        }
      ],
      "source": [
        "%cd LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163e9e43-f375-4999-9cdb-d09c163f5a7a",
      "metadata": {
        "id": "163e9e43-f375-4999-9cdb-d09c163f5a7a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install -e .\n",
        "# # !pip install -e . #to see the install logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c36f749a-1ce0-490b-b5b9-9de6423561f7",
      "metadata": {
        "id": "c36f749a-1ce0-490b-b5b9-9de6423561f7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # !git pull\n",
        "# # !pip install -e . -q\n",
        "\n",
        "# !pip install protobuf -q -U\n",
        "# !pip install --upgrade Pillow -q\n",
        "# !pip install -e \".[train]\" -q\n",
        "# !pip install flash-attn --no-build-isolation -q\n",
        "\n",
        "# # IT'S RECOMMENDED TO RESTART THE KERNEL TO ENSURE UPDATED PACKAGES ARE LOADED, THEN RUN THE CELLS BELOW"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01dfda3c-2d31-492b-b89b-5e99fff7d5e5",
      "metadata": {
        "id": "01dfda3c-2d31-492b-b89b-5e99fff7d5e5"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7e53635-67ac-49e9-8962-35e0fcad8cfb",
      "metadata": {
        "id": "e7e53635-67ac-49e9-8962-35e0fcad8cfb",
        "outputId": "465f3a84-d3e6-4e9c-f738-45d46f1f49fb",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_HUB_ENABLE_HF_TRANSFER=1\n"
          ]
        }
      ],
      "source": [
        "# Allows for faster downloading.\n",
        "%env HF_HUB_ENABLE_HF_TRANSFER=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29f08362-066e-400c-98fc-d7e123194cf4",
      "metadata": {
        "id": "29f08362-066e-400c-98fc-d7e123194cf4",
        "outputId": "2c2fb89e-866a-4eb8-c40e-e9cd54c68433",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2024-04-28 11:03:11,139] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from PIL import Image\n",
        "import transformers\n",
        "from transformers import AutoProcessor, Trainer, TrainingArguments, BitsAndBytesConfig\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44128b4e-b992-48ca-a82a-5f54e9a8314d",
      "metadata": {
        "id": "44128b4e-b992-48ca-a82a-5f54e9a8314d",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# !pip install flash-attn --no-build-isolation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f36c83b-1e06-4478-adbd-7698b8284e77",
      "metadata": {
        "id": "8f36c83b-1e06-4478-adbd-7698b8284e77",
        "outputId": "07714034-aaf5-408a-b5f1-d329a3bdc50c",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/opt/conda/envs/test_install/bin/python'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import sys\n",
        "# sys.executable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf47a4c3-41f4-498d-af35-c075b0b7c080",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "5f197a8aecb3436680d907de6cae13bb"
          ]
        },
        "id": "cf47a4c3-41f4-498d-af35-c075b0b7c080",
        "outputId": "24a8b270-1592-43ee-a4c4-cead7ed6c89b",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/nn/modules/module.py:2025: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
            "  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f197a8aecb3436680d907de6cae13bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vision_tower is loaded\n"
          ]
        }
      ],
      "source": [
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.mm_utils import get_model_name_from_path\n",
        "from llava.eval.run_llava import eval_model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# LLaVA 1.6B\n",
        "model_path = \"liuhaotian/llava-v1.6-vicuna-7b\"\n",
        "\n",
        "model_name=get_model_name_from_path(model_path)\n",
        "\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path=model_path,\n",
        "    model_base=None,\n",
        "    model_name=model_name,\n",
        "    cache_dir='',\n",
        "    use_flash_attn=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2fbccbe-5639-4749-a71d-a8cf6a5fad38",
      "metadata": {
        "id": "d2fbccbe-5639-4749-a71d-a8cf6a5fad38"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5381d4ba-437a-461e-991a-41c0fe39592e",
      "metadata": {
        "id": "5381d4ba-437a-461e-991a-41c0fe39592e",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import sys\n",
        "# !{sys.executable} -m pip install flash-attn --no-build-isolation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f6703ec-5f00-4391-a1eb-2fc44b76bd23",
      "metadata": {
        "id": "3f6703ec-5f00-4391-a1eb-2fc44b76bd23",
        "outputId": "ca1540dd-da9a-4e12-d7ca-6a7a3bac762c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to import flash_attn: No module named 'flash_attn'\n"
          ]
        }
      ],
      "source": [
        "# try:\n",
        "#     import flash_attn\n",
        "#     print(\"flash_attn successfully imported!\")\n",
        "# except ImportError as e:\n",
        "#     print(f\"Failed to import flash_attn: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5171ff7-28a4-4e96-8b60-b1051308c090",
      "metadata": {
        "id": "b5171ff7-28a4-4e96-8b60-b1051308c090"
      },
      "outputs": [],
      "source": [
        "# !{sys.executable} -m pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7308fe4-ec7e-4aab-95b1-71bb9550f7fb",
      "metadata": {
        "id": "c7308fe4-ec7e-4aab-95b1-71bb9550f7fb"
      },
      "source": [
        "Examination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91ef2136-1a44-4ef5-9a22-17e98d8f7db6",
      "metadata": {
        "id": "91ef2136-1a44-4ef5-9a22-17e98d8f7db6",
        "outputId": "2952cebb-ce28-4051-be28-e4738938c68c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlavaLlamaForCausalLM(\n",
            "  (model): LlavaLlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaFlashAttention2(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "    (vision_tower): CLIPVisionTower(\n",
            "      (vision_tower): CLIPVisionModel(\n",
            "        (vision_model): CLIPVisionTransformer(\n",
            "          (embeddings): CLIPVisionEmbeddings(\n",
            "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
            "            (position_embedding): Embedding(577, 1024)\n",
            "          )\n",
            "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder): CLIPEncoder(\n",
            "            (layers): ModuleList(\n",
            "              (0-23): 24 x CLIPEncoderLayer(\n",
            "                (self_attn): CLIPAttention(\n",
            "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                )\n",
            "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (mlp): CLIPMLP(\n",
            "                  (activation_fn): QuickGELUActivation()\n",
            "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "                )\n",
            "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (mm_projector): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9886c362-b41b-4fca-b2da-1147659cc324",
      "metadata": {
        "id": "9886c362-b41b-4fca-b2da-1147659cc324",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# print(processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "460afc62-24d1-4e02-ac61-af81a94d06bc",
      "metadata": {
        "id": "460afc62-24d1-4e02-ac61-af81a94d06bc",
        "outputId": "e04a9dc7-1a82-4465-8acb-49bab7ac4c93",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlavaConfig {\n",
            "  \"_name_or_path\": \"liuhaotian/llava-v1.6-vicuna-7b\",\n",
            "  \"architectures\": [\n",
            "    \"LlavaLlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"freeze_mm_mlp_adapter\": false,\n",
            "  \"freeze_mm_vision_resampler\": false,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"image_aspect_ratio\": \"anyres\",\n",
            "  \"image_crop_resolution\": 224,\n",
            "  \"image_grid_pinpoints\": [\n",
            "    [\n",
            "      336,\n",
            "      672\n",
            "    ],\n",
            "    [\n",
            "      672,\n",
            "      336\n",
            "    ],\n",
            "    [\n",
            "      672,\n",
            "      672\n",
            "    ],\n",
            "    [\n",
            "      1008,\n",
            "      336\n",
            "    ],\n",
            "    [\n",
            "      336,\n",
            "      1008\n",
            "    ]\n",
            "  ],\n",
            "  \"image_split_resolution\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 11008,\n",
            "  \"max_position_embeddings\": 4096,\n",
            "  \"mm_hidden_size\": 1024,\n",
            "  \"mm_patch_merge_type\": \"spatial_unpad\",\n",
            "  \"mm_projector_lr\": null,\n",
            "  \"mm_projector_type\": \"mlp2x_gelu\",\n",
            "  \"mm_resampler_type\": null,\n",
            "  \"mm_use_im_patch_token\": false,\n",
            "  \"mm_use_im_start_end\": false,\n",
            "  \"mm_vision_select_feature\": \"patch\",\n",
            "  \"mm_vision_select_layer\": -2,\n",
            "  \"mm_vision_tower\": \"openai/clip-vit-large-patch14-336\",\n",
            "  \"mm_vision_tower_lr\": 2e-06,\n",
            "  \"model_type\": \"llava_llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_model_max_length\": 4096,\n",
            "  \"tokenizer_padding_side\": \"right\",\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"tune_mm_mlp_adapter\": false,\n",
            "  \"tune_mm_vision_resampler\": false,\n",
            "  \"unfreeze_mm_vision_tower\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"use_mm_proj\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "0\n",
            "LlamaTokenizer(name_or_path='liuhaotian/llava-v1.6-vicuna-7b', vocab_size=32000, model_max_length=4096, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print(model.config)\n",
        "print(tokenizer.pad_token_id)\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a8dfbd0-c0bf-4e19-adc1-0f5909413c86",
      "metadata": {
        "id": "4a8dfbd0-c0bf-4e19-adc1-0f5909413c86",
        "outputId": "102a0ffe-4173-49b2-97fe-2f12323ba403",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modules not torch.bfloat16:\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Check dtype of all modules, focusing on those not torch.bfloat16\n",
        "print(\"Modules not torch.bfloat16:\")\n",
        "for name, module in model.named_modules():\n",
        "    if hasattr(module, 'parameters') and list(module.parameters()):\n",
        "        # Check if any parameter of the module is not bfloat16\n",
        "        if any(param.dtype != torch.bfloat16 for param in module.parameters()):\n",
        "            print(f\"{name}: {next(module.parameters()).dtype}\")\n",
        "    else:\n",
        "        # Optionally, acknowledge modules without parameters if needed\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f382b7a-ed8e-4066-871f-7dc717eb639b",
      "metadata": {
        "id": "7f382b7a-ed8e-4066-871f-7dc717eb639b",
        "outputId": "4623565c-821b-4cbf-d80d-1a8fb46a7f58",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "transformers version: 4.37.2\n",
            "accelerate version: 0.21.0\n",
            "torch version: 2.1.2+cu121\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import accelerate\n",
        "import torch\n",
        "\n",
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(f\"accelerate version: {accelerate.__version__}\")\n",
        "print(f\"torch version: {torch.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "403e8f93-f50a-43cd-bcb4-410ab4edd657",
      "metadata": {
        "id": "403e8f93-f50a-43cd-bcb4-410ab4edd657",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# !pip install torch==2.1.2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40e7f3be-25c5-4d53-ae3c-fd8d44460fb5",
      "metadata": {
        "id": "40e7f3be-25c5-4d53-ae3c-fd8d44460fb5",
        "tags": []
      },
      "source": [
        "## Image loading and processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f0837d7-0864-45fd-be49-ac323fd30795",
      "metadata": {
        "id": "4f0837d7-0864-45fd-be49-ac323fd30795",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "from llava.constants import (\n",
        "    IMAGE_TOKEN_INDEX,\n",
        "    DEFAULT_IMAGE_TOKEN,\n",
        "    DEFAULT_IM_START_TOKEN,\n",
        "    DEFAULT_IM_END_TOKEN,\n",
        "    IMAGE_PLACEHOLDER,\n",
        ")\n",
        "from llava.conversation import conv_templates, SeparatorStyle\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.utils import disable_torch_init\n",
        "from llava.mm_utils import (\n",
        "    process_images,\n",
        "    tokenizer_image_token,\n",
        "    get_model_name_from_path,\n",
        ")\n",
        "\n",
        "# Common function to create prompts\n",
        "def create_prompt(query, model, model_name=model_name, caption=None):\n",
        "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
        "    if IMAGE_PLACEHOLDER in query:\n",
        "        if model.config.mm_use_im_start_end:\n",
        "            query = re.sub(IMAGE_PLACEHOLDER, image_token_se, query)\n",
        "        else:\n",
        "            query = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, query)\n",
        "    else:\n",
        "        if model.config.mm_use_im_start_end:\n",
        "            query = image_token_se + \"\\n\" + query\n",
        "        else:\n",
        "            query = DEFAULT_IMAGE_TOKEN + \"\\n\" + query\n",
        "\n",
        "    # print(f\"Prompt: in create prompt {query}\")\n",
        "\n",
        "    conv_mode = infer_conv_mode(model_name)\n",
        "    conv = conv_templates[conv_mode].copy()\n",
        "    conv.append_message(conv.roles[0], query)\n",
        "    if caption is not None:\n",
        "        conv.append_message(conv.roles[1], caption)\n",
        "    else:\n",
        "        conv.append_message(conv.roles[1], None)\n",
        "    return conv.get_prompt()\n",
        "\n",
        "# Common function to infer conversation mode\n",
        "def infer_conv_mode(model_name):\n",
        "    if \"llama-2\" in model_name.lower():\n",
        "        return \"llava_llama_2\"\n",
        "    elif \"mistral\" in model_name.lower():\n",
        "        return \"mistral_instruct\"\n",
        "    elif \"v1.6-34b\" in model_name.lower():\n",
        "        return \"chatml_direct\"\n",
        "    elif \"v1\" in model_name.lower():\n",
        "        return \"llava_v1\"\n",
        "    elif \"mpt\" in model_name.lower():\n",
        "        return \"mpt\"\n",
        "    else:\n",
        "        return \"llava_v0\"\n",
        "\n",
        "def load_image(image_input):\n",
        "    # Check if the input is a string (path or URL)\n",
        "    if isinstance(image_input, str):\n",
        "        if image_input.startswith(\"http\") or image_input.startswith(\"https\"):\n",
        "            response = requests.get(image_input)\n",
        "            image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "        else:\n",
        "            image = Image.open(image_input).convert(\"RGB\")\n",
        "    elif isinstance(image_input, Image.Image):\n",
        "        # Input is already an Image object, return as is\n",
        "        image = image_input\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported image input type\")\n",
        "    return image\n",
        "\n",
        "def load_images(image_files):\n",
        "    out = []\n",
        "    for image_file in image_files:\n",
        "        image = load_image(image_file)\n",
        "        out.append(image)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f07ebd25-d62a-4f0c-9782-032c228e5f30",
      "metadata": {
        "id": "f07ebd25-d62a-4f0c-9782-032c228e5f30",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def process_images(images, image_processor, model_config):\n",
        "    processed_images = []\n",
        "    for image in images:\n",
        "        processed_output = image_processor(image)\n",
        "        # Check the type of 'pixel_values'; if it's a list, convert it to a tensor\n",
        "        if isinstance(processed_output['pixel_values'], list):\n",
        "            tensor = torch.tensor(processed_output['pixel_values'])\n",
        "        else:\n",
        "            tensor = processed_output['pixel_values']\n",
        "        processed_images.append(tensor)\n",
        "\n",
        "    # Stack the processed images into a single tensor\n",
        "    images_tensor = torch.stack(processed_images, dim=0)\n",
        "\n",
        "    return images_tensor\n",
        "\n",
        "\n",
        "def process_and_prepare_images(image_files, image_processor, model, device):\n",
        "    images = load_images(image_files)\n",
        "    image_sizes = [x.size for x in images]  # Calculate image sizes once\n",
        "\n",
        "    # Process images and ensure the output is a tensor\n",
        "    images_tensor = process_images(\n",
        "        images,\n",
        "        image_processor,\n",
        "        model.config\n",
        "    ).to(\n",
        "        device,\n",
        "        dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    return images_tensor, image_sizes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feeb1a79-7b58-4a2b-b28a-040f7819e212",
      "metadata": {
        "id": "feeb1a79-7b58-4a2b-b28a-040f7819e212",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # Assuming `image_processor` is your image processing function and `sample_image` is a single image loaded into memory\n",
        "# processed_output = image_processor(sample_image)\n",
        "\n",
        "# # Print the keys in the processed output\n",
        "# print(processed_output.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbfc2831-23d8-4a0c-b75b-054241968292",
      "metadata": {
        "id": "cbfc2831-23d8-4a0c-b75b-054241968292"
      },
      "source": [
        "## eval model function for generating captions via evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4435cf0d-936a-4e9f-8f78-44e7b4846517",
      "metadata": {
        "id": "4435cf0d-936a-4e9f-8f78-44e7b4846517",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import re\n",
        "\n",
        "# def eval_model(tokenizer, model, image_processor, context_len, image_file, query, model_name=model_name, sep=\",\", temperature=1.0, num_beams=1, max_new_tokens=150):\n",
        "#     # Model\n",
        "#     disable_torch_init()\n",
        "\n",
        "#     # Create prompt using the common function\n",
        "#     prompt = create_prompt(query, model, model_name)\n",
        "\n",
        "#     print(f\"Prompt: {prompt}\")\n",
        "\n",
        "#     # Process images using the common function\n",
        "#     if isinstance(image_file, list):\n",
        "#         images_tensor, image_sizes = process_and_prepare_images(image_file, image_processor, model, model.device)\n",
        "#     elif isinstance(image_file, str):\n",
        "#         images_tensor, image_sizes = process_and_prepare_images([image_file], image_processor, model, model.device)\n",
        "#     else:\n",
        "#         # If image_file is neither a list nor a string, it's likely an Image object or similar; wrap it in a list\n",
        "#         images = [image_file]\n",
        "#         images_tensor, image_sizes = process_and_prepare_images(images, image_processor, model, model.device)\n",
        "\n",
        "#     # Tokenize the prompt using the custom tokenizer_image_token function\n",
        "#     input_ids = (\n",
        "#         tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
        "#         .unsqueeze(0)\n",
        "#         .to(model.device)\n",
        "#     )\n",
        "\n",
        "#     # print(images_tensor.size())\n",
        "\n",
        "#     with torch.inference_mode():\n",
        "#         output_ids = model.generate(\n",
        "#             input_ids,\n",
        "#             images=images_tensor,\n",
        "#             image_sizes=image_sizes,\n",
        "#             do_sample=temperature != 1.0,\n",
        "#             temperature=temperature,\n",
        "#             # top_p=top_p,\n",
        "#             num_beams=num_beams,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             use_cache=True,\n",
        "#         )\n",
        "\n",
        "#     outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=False)[0].strip()\n",
        "#     print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee6eb95-9e37-4984-a2c3-7da7c3e2b173",
      "metadata": {
        "id": "3ee6eb95-9e37-4984-a2c3-7da7c3e2b173",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "\n",
        "def eval_model(tokenizer, model, image_processor, context_len, image_file, query, model_name=model_name, sep=\",\", temperature=1.0, num_beams=1, max_new_tokens=150):\n",
        "    # Model setup\n",
        "    disable_torch_init()\n",
        "\n",
        "    # Create prompt using the common function\n",
        "    prompt = create_prompt(query, model, model_name)\n",
        "\n",
        "    # Process images using the common function\n",
        "    if isinstance(image_file, list):\n",
        "        images_tensor, image_sizes = process_and_prepare_images(image_file, image_processor, model, model.device)\n",
        "    elif isinstance(image_file, str):\n",
        "        images_tensor, image_sizes = process_and_prepare_images([image_file], image_processor, model, model.device)\n",
        "    else:\n",
        "        # If image_file is neither a list nor a string, it's likely an Image object or similar; wrap it in a list\n",
        "        images = [image_file]\n",
        "        images_tensor, image_sizes = process_and_prepare_images(images, image_processor, model, model.device)\n",
        "\n",
        "    # Tokenize the prompt using the custom tokenizer_image_token function\n",
        "    input_ids = (\n",
        "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
        "        .unsqueeze(0)\n",
        "        .to(model.device)\n",
        "    )\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            images=images_tensor,\n",
        "            image_sizes=image_sizes,\n",
        "            do_sample=temperature != 1.0,\n",
        "            temperature=temperature,\n",
        "            num_beams=num_beams,    #this decides koita sequence generate hobe, num_beams = 1 disi so 1ta sequence generate hobe\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "    # Decode the output, removing special tokens for cleaner captions\n",
        "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a027c4-0c63-4085-be74-4de1c7413bd3",
      "metadata": {
        "id": "b7a027c4-0c63-4085-be74-4de1c7413bd3"
      },
      "source": [
        "### Method to test the model's inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4c56471-fcca-4586-9c47-978cb4320d8a",
      "metadata": {
        "id": "a4c56471-fcca-4586-9c47-978cb4320d8a",
        "outputId": "93b8a451-6376-42c9-d086-6734aea4eac6",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/jupyter/LLaVA'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ddb6ff-2062-4f58-aadb-16b7269f376c",
      "metadata": {
        "id": "13ddb6ff-2062-4f58-aadb-16b7269f376c",
        "outputId": "c513831a-e824-4e01-f858-ff7b90626d8f",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt: A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions. USER: <image>\n",
            "What do you see in this picture? ASSISTANT:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/tmp/ipykernel_8579/4265429433.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  tensor = torch.tensor(processed_output['pixel_values'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> The image appears to be a medical scan, likely an X-ray or CT scan, showing the abdominal area of a person. There are several areas highlighted with arrows, which are typically used to indicate areas of interest or concern. The red arrow points to a region that might be of interest to a medical professional, possibly indicating a mass or an area of concern that requires further investigation. The other arrows seem to be pointing to other areas of interest, but without additional context, it's difficult to determine what they are specifically highlighting. It's important to note that medical imaging should only be interpreted by qualified medical professionals.</s>\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "image_url = '/home/jupyter/imageCLEF 2024 subset/Train/ImageCLEFmedical_Caption_2024_train_000038.jpg'\n",
        "image = Image.open(image_url).convert(\"RGB\")\n",
        "\n",
        "\n",
        "\n",
        "image=load_image(image_url)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# Display the image using matplotlib\n",
        "plt.imshow(image)\n",
        "plt.axis('off')  # Turn off axis numbers and ticks\n",
        "plt.show()\n",
        "\n",
        "# pass the processed image to eval_model\n",
        "eval_model(\n",
        "    tokenizer,\n",
        "    model,\n",
        "    image_processor,\n",
        "    context_len,\n",
        "    image,  # Use the processed image\n",
        "    \"What do you see in this picture?\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c18686b-7771-4e3c-89f0-6b4fbe72a6f0",
      "metadata": {
        "id": "0c18686b-7771-4e3c-89f0-6b4fbe72a6f0"
      },
      "outputs": [],
      "source": [
        "# os.chdir('/home/jupyter/LLaVA')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140fed7c-c5ce-4d04-a610-0ec376733ffc",
      "metadata": {
        "id": "140fed7c-c5ce-4d04-a610-0ec376733ffc",
        "outputId": "af241e77-1ef3-4143-bf6d-5667fa01a227",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/jupyter/LLaVA'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9006832-ac0f-4603-b4f2-7407684f203c",
      "metadata": {
        "id": "a9006832-ac0f-4603-b4f2-7407684f203c"
      },
      "source": [
        "# Finetuning dataset\n",
        "\n",
        "Prepare the dataset that will be used for finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd73a79-8fd9-435e-a446-8cd3b9aad0ab",
      "metadata": {
        "id": "7fd73a79-8fd9-435e-a446-8cd3b9aad0ab",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict, Features, Value, Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37ed7a62-fa03-4e92-97f8-c40371dafc03",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "fab4aa9dc47f4b5d9db907b399cda82f"
          ]
        },
        "id": "37ed7a62-fa03-4e92-97f8-c40371dafc03",
        "outputId": "b7891b2a-0e20-4dd2-eadf-04b391d08f1a",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fab4aa9dc47f4b5d9db907b399cda82f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebd78227-edc4-4245-b467-dd08f7f2448c",
      "metadata": {
        "id": "ebd78227-edc4-4245-b467-dd08f7f2448c",
        "outputId": "f2e7f953-fab2-4e69-a05d-614b5c7db351"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Parameter 'transform'=<function transform_batch at 0x7f3c66fbee50> of the transform datasets.arrow_dataset.Dataset.set_format couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def tokenize_and_create_labels(example_batch, image_processor, tokenizer, model, device, model_name=model_name, ignore_index=-100, image_token_index=IMAGE_TOKEN_INDEX):\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    image_files = example_batch['image']\n",
        "\n",
        "    images_tensor, image_sizes = process_and_prepare_images(image_files, image_processor, model, device)\n",
        "\n",
        "    query = \"What do you see in this picture?\"\n",
        "\n",
        "    # Tokenize the conversation without the captions to determine which tokens to ignore\n",
        "    tokenized_conversations_without_caption = [\n",
        "        tokenizer_image_token(create_prompt(query, model, model_name, None), tokenizer, image_token_index, return_tensors=\"pt\")\n",
        "        for _ in example_batch['caption']\n",
        "    ]\n",
        "\n",
        "    # Tokenize the full conversations with the captions\n",
        "    tokenized_conversations_with_caption = [\n",
        "        tokenizer_image_token(create_prompt(query, model, model_name, caption), tokenizer, image_token_index, return_tensors=\"pt\")\n",
        "        for caption in example_batch['caption']\n",
        "    ]\n",
        "\n",
        "    # Pad the tokenized conversations to the same length\n",
        "    input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_with_caption], batch_first=True, padding_value=pad_token_id).to(device)\n",
        "\n",
        "    # Create attention_mask (1 for real tokens and 0 for padding tokens)\n",
        "    attention_mask = (input_ids != pad_token_id).long().to(device)\n",
        "\n",
        "    # Create the labels tensor which is a copy of input_ids but with ignore_index for non-caption tokens\n",
        "    labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
        "    for i, tcwc in enumerate(tokenized_conversations_without_caption):\n",
        "        # Set ignore_index for the tokens corresponding to the conversation without the caption\n",
        "        input_id_without_caption = tcwc.squeeze(0)\n",
        "        labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
        "\n",
        "    inputs = {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"images\": images_tensor,\n",
        "        \"image_sizes\": image_sizes,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Make sure to define the function outside of the lambda to ensure it's serializable\n",
        "def transform_batch(batch):\n",
        "    return tokenize_and_create_labels(batch, image_processor, tokenizer, model, device, model_name)\n",
        "\n",
        "# load and prepare dataset\n",
        "ds = load_dataset(\"HoqueMahmudul/clef2024_train_valid_a\")\n",
        "\n",
        "train_ds = ds[\"train\"]\n",
        "eval_ds = ds[\"test\"]\n",
        "\n",
        "# print(train_ds)\n",
        "# print(eval_ds)\n",
        "\n",
        "# Apply the transformation function to the dataset\n",
        "train_ds.set_transform(transform_batch)\n",
        "eval_ds.set_transform(transform_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d24b98-e84e-4af6-a36f-3e5a52dbcf5f",
      "metadata": {
        "id": "f3d24b98-e84e-4af6-a36f-3e5a52dbcf5f"
      },
      "source": [
        "#### Addressing the Attention Mask Warning and Pad Token ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a460c2-3d2e-4cf5-b292-6ea742881140",
      "metadata": {
        "id": "45a460c2-3d2e-4cf5-b292-6ea742881140"
      },
      "outputs": [],
      "source": [
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# def tokenize_and_create_labels(example_batch, image_processor, tokenizer, model, device, model_name=model_name, ignore_index=-100, image_token_index=IMAGE_TOKEN_INDEX):\n",
        "#     pad_token_id = tokenizer.pad_token_id\n",
        "#     image_files = example_batch['image']\n",
        "\n",
        "#     images_tensor, image_sizes = process_and_prepare_images(image_files, image_processor, model, device)\n",
        "\n",
        "#     query = \"What do you see in this picture?\"\n",
        "\n",
        "#     # Tokenize the conversation without the captions to determine which tokens to ignore\n",
        "#     tokenized_conversations_without_caption = [\n",
        "#         tokenizer_image_token(create_prompt(query, model, model_name, None), tokenizer, image_token_index, return_tensors=\"pt\")\n",
        "#         for _ in example_batch['caption']\n",
        "#     ]\n",
        "\n",
        "#     # Tokenize the full conversations with the captions\n",
        "#     tokenized_conversations_with_caption = [\n",
        "#         tokenizer_image_token(create_prompt(query, model, model_name, caption), tokenizer, image_token_index, return_tensors=\"pt\")\n",
        "#         for caption in example_batch['caption']\n",
        "#     ]\n",
        "\n",
        "#     # Pad the tokenized conversations to the same length\n",
        "#     input_ids = pad_sequence([tcwc.squeeze(0) for tcwc in tokenized_conversations_with_caption], batch_first=True, padding_value=pad_token_id).to(device)\n",
        "\n",
        "#     # Create attention_mask (1 for real tokens and 0 for padding tokens)\n",
        "#     attention_mask = (input_ids != pad_token_id).long().to(device)\n",
        "\n",
        "#     # Create the labels tensor which is a copy of input_ids but with ignore_index for non-caption tokens\n",
        "#     labels = torch.full_like(input_ids, fill_value=ignore_index)\n",
        "#     for i, tcwc in enumerate(tokenized_conversations_without_caption):\n",
        "#         # Set ignore_index for the tokens corresponding to the conversation without the caption\n",
        "#         input_id_without_caption = tcwc.squeeze(0)\n",
        "#         labels[i, len(input_id_without_caption):] = input_ids[i, len(input_id_without_caption):]\n",
        "\n",
        "#     inputs = {\n",
        "#         \"input_ids\": input_ids,\n",
        "#         \"attention_mask\": attention_mask,\n",
        "#         \"images\": images_tensor,\n",
        "#         \"image_sizes\": image_sizes,\n",
        "#         \"labels\": labels,\n",
        "#     }\n",
        "\n",
        "#     return inputs\n",
        "\n",
        "# # Make sure to define the function outside of the lambda to ensure it's serializable\n",
        "# def transform_batch(batch):\n",
        "#     return tokenize_and_create_labels(batch, image_processor, tokenizer, model, device, model_name)\n",
        "\n",
        "# # load and prepare dataset\n",
        "# ds = load_dataset(\"HoqueMahmudul/test\")\n",
        "\n",
        "# train_ds = ds[\"train\"]\n",
        "# eval_ds = ds[\"test\"]\n",
        "\n",
        "# # print(train_ds)\n",
        "# # print(eval_ds)\n",
        "\n",
        "# # Apply the transformation function to the dataset\n",
        "# train_ds.set_transform(transform_batch)\n",
        "# eval_ds.set_transform(transform_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7405552a-13d4-4f00-8a6f-ec55cb7686aa",
      "metadata": {
        "id": "7405552a-13d4-4f00-8a6f-ec55cb7686aa"
      },
      "outputs": [],
      "source": [
        "# for batch in eval_ds:\n",
        "#     print(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f5b3d47-f055-4d62-b32a-cbc72149ea91",
      "metadata": {
        "id": "1f5b3d47-f055-4d62-b32a-cbc72149ea91"
      },
      "source": [
        "# LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91408fba-7dae-402c-8231-b24689af6520",
      "metadata": {
        "id": "91408fba-7dae-402c-8231-b24689af6520",
        "outputId": "692cacfe-4179-4180-db32-00cd2d511db6",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlavaMistralForCausalLM(\n",
            "  (model): LlavaMistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "    (vision_tower): CLIPVisionTower(\n",
            "      (vision_tower): CLIPVisionModel(\n",
            "        (vision_model): CLIPVisionTransformer(\n",
            "          (embeddings): CLIPVisionEmbeddings(\n",
            "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
            "            (position_embedding): Embedding(577, 1024)\n",
            "          )\n",
            "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder): CLIPEncoder(\n",
            "            (layers): ModuleList(\n",
            "              (0-23): 24 x CLIPEncoderLayer(\n",
            "                (self_attn): CLIPAttention(\n",
            "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                )\n",
            "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (mlp): CLIPMLP(\n",
            "                  (activation_fn): QuickGELUActivation()\n",
            "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "                )\n",
            "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (mm_projector): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Mistral\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5cd4734-9b9e-44c2-9dd2-15dbd0b02d34",
      "metadata": {
        "id": "f5cd4734-9b9e-44c2-9dd2-15dbd0b02d34",
        "outputId": "b45e541c-07b1-41be-9b42-11bc45b96768",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LlavaLlamaForCausalLM(\n",
            "  (model): LlavaLlamaModel(\n",
            "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x LlamaDecoderLayer(\n",
            "        (self_attn): LlamaSdpaAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): LlamaRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): LlamaMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): LlamaRMSNorm()\n",
            "        (post_attention_layernorm): LlamaRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): LlamaRMSNorm()\n",
            "    (vision_tower): CLIPVisionTower(\n",
            "      (vision_tower): CLIPVisionModel(\n",
            "        (vision_model): CLIPVisionTransformer(\n",
            "          (embeddings): CLIPVisionEmbeddings(\n",
            "            (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
            "            (position_embedding): Embedding(577, 1024)\n",
            "          )\n",
            "          (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "          (encoder): CLIPEncoder(\n",
            "            (layers): ModuleList(\n",
            "              (0-23): 24 x CLIPEncoderLayer(\n",
            "                (self_attn): CLIPAttention(\n",
            "                  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                )\n",
            "                (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                (mlp): CLIPMLP(\n",
            "                  (activation_fn): QuickGELUActivation()\n",
            "                  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "                )\n",
            "                (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (mm_projector): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "      (1): GELU(approximate='none')\n",
            "      (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Vicuna-llama\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "603ed22c-3f51-4a82-83d8-dcc0cbbc0ff5",
      "metadata": {
        "id": "603ed22c-3f51-4a82-83d8-dcc0cbbc0ff5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha = 32 * math.sqrt(32), # this will set the rescaled learning rate to LR * alpha / sqrt (r), with r the rank.\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", # attention mechanisms\n",
        "        \"fc1\", \"fc2\", #for llama,\n",
        "        \"mm_projector.0\", #for mistral, train instead \"mm_projector\" - Adapt the multimodal projection layer for better visual-textual alignment\n",
        "        \"mm_projector.2\",\n",
        "        \"up_proj\", \"down_proj\",\"gate_proj\" #Adapt MLP components within transformer blocks, optionally train more linear layers\n",
        "        ],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        ")\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6176938c-a2eb-4f5e-a2b3-101af2eaffcb",
      "metadata": {
        "id": "6176938c-a2eb-4f5e-a2b3-101af2eaffcb",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# List to hold the names of the trainable parameters\n",
        "set_other_trainable=True\n",
        "\n",
        "if set_other_trainable:\n",
        "    trainable_params_names = ['lm_head','embed_tokens']\n",
        "    # trainable_params_names = None\n",
        "\n",
        "    # Set modules to be trainable\n",
        "    for n, p in model.named_parameters():\n",
        "        if any(k in n for k in trainable_params_names):\n",
        "            p.requires_grad_(True)\n",
        "        # else:\n",
        "        #     p.requires_grad_(False)  # Optional: Set the rest to be not trainable\n",
        "\n",
        "    # Make a dictionary of trainable parameters\n",
        "    trainable_params = {n: p for n, p in model.named_parameters() if p.requires_grad}\n",
        "\n",
        "    # Convert trainable_params to state_dict format\n",
        "    trainable_params_state_dict = {n: p.data for n, p in trainable_params.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc79711a-ecb4-4369-9b93-9fa3555e2a5b",
      "metadata": {
        "id": "fc79711a-ecb4-4369-9b93-9fa3555e2a5b",
        "outputId": "749f4cb2-ca07-4cf7-84be-d036397e1a5c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 346,718,208 || all params: 7,147,481,088 || trainable%: 4.850914661139989\n"
          ]
        }
      ],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b726e40f-b419-41aa-b15d-97c8b546d293",
      "metadata": {
        "id": "b726e40f-b419-41aa-b15d-97c8b546d293"
      },
      "source": [
        "# Pre-Training Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c8ad10-adab-4e01-a96a-b7798218a02a",
      "metadata": {
        "id": "a0c8ad10-adab-4e01-a96a-b7798218a02a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0dac1641-08e9-45cb-a719-28e450885c19",
      "metadata": {
        "id": "0dac1641-08e9-45cb-a719-28e450885c19",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Temporarily disable the transformation to access the original data\n",
        "# eval_ds.reset_format()\n",
        "\n",
        "# # Limit the number of images to process\n",
        "# num_images_to_process = 5\n",
        "\n",
        "# # Iterate over each example in the evaluation dataset, but only up to num_images_to_process\n",
        "# for i in range(min(num_images_to_process, len(eval_ds))):\n",
        "#     # Access the original image and caption for the current row\n",
        "#     image = eval_ds[i]['image']\n",
        "#     caption = eval_ds[i]['caption']\n",
        "\n",
        "#     # Display the image using matplotlib\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')  # Turn off axis numbers and ticks\n",
        "#     plt.show()\n",
        "\n",
        "#     # Evaluate the model for the given image\n",
        "#     eval_model(\n",
        "#         tokenizer,\n",
        "#         model,\n",
        "#         image_processor,\n",
        "#         context_len,\n",
        "#         image,\n",
        "#         \"What do you see in this picture?\"\n",
        "#     )\n",
        "\n",
        "#     # Print the correct caption\n",
        "#     print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
        "\n",
        "# # Re-enable the transformation if needed\n",
        "# eval_ds.set_transform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33a7e276-bd1d-4419-a12e-07be0c8c8502",
      "metadata": {
        "id": "33a7e276-bd1d-4419-a12e-07be0c8c8502"
      },
      "source": [
        "# Training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d5e45d-b587-491c-8155-66d84158df84",
      "metadata": {
        "id": "92d5e45d-b587-491c-8155-66d84158df84",
        "tags": []
      },
      "outputs": [],
      "source": [
        "batch_size = 4  # Specify the batch size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a82f402c-c3ef-4d85-ba21-cb1c86a7e113",
      "metadata": {
        "id": "a82f402c-c3ef-4d85-ba21-cb1c86a7e113",
        "outputId": "2b1e902c-1e12-454e-81e9-290661724f6d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/tmp/ipykernel_10744/4265429433.py:22: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  tensor = torch.tensor(processed_output['pixel_values'])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['input_ids', 'attention_mask', 'images', 'image_sizes', 'labels'])\n",
            "Images are included in the DataLoader.\n",
            "Batch 'images' shape: torch.Size([4, 1, 3, 336, 336])\n",
            "First row of 'input_ids': \n",
            "[1, 319, 13563, 1546, 263, 12758, 5199, 322, 385, 23116, 21082, 20255, 29889, 450, 20255, 4076, 8444, 29892, 13173, 29892, 322, 1248, 568, 6089, 304, 278, 5199, 29915, 29879, 5155, 29889, 3148, 1001, 29901, 29871, -200, 29871, 13, 5618, 437, 366, 1074, 297, 445, 7623, 29973, 319, 1799, 9047, 13566, 29901, 12699, 284, 1776, 310, 2343, 322, 315, 29899, 1028, 457, 515, 11247, 3970, 29990, 6445, 385, 633, 8945, 315, 29906, 24241, 1182, 284, 3573, 411, 10757, 310, 1014, 29880, 1314, 362, 29889, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Text inputs are included in the DataLoader.\n",
            "Batch 'input_ids' shape: torch.Size([4, 104])\n",
            "Batch 'attention_mask' shape: torch.Size([4, 104])\n",
            "Labels: ['[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '[IGNORE]', '12699', '284', '1776', '310', '2343', '322', '315', '29899', '1028', '457', '515', '11247', '3970', '29990', '6445', '385', '633', '8945', '315', '29906', '24241', '1182', '284', '3573', '411', '10757', '310', '1014', '29880', '1314', '362', '29889', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']\n",
            "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "First Row Image Data type: torch.bfloat16\n",
            "First Row Image Shape: torch.Size([1, 3, 336, 336])\n",
            "First Row Image Value range: [-1.7890625, 2.140625]\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assuming train_ds is your training dataset prepared as a PyTorch Dataset object\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Assuming train_loader is your DataLoader instance for the training dataset\n",
        "for batch in train_loader:\n",
        "    print(batch.keys())  # Print the dictionary keys to see what data is included in a batch\n",
        "\n",
        "    # If 'images' is a key, this indicates that images are being loaded\n",
        "    if 'images' in batch:\n",
        "        print(\"Images are included in the DataLoader.\")\n",
        "        print(f\"Batch 'images' shape: {batch['images'].shape}\")  # Print the shape of the images tensor\n",
        "\n",
        "    # Similarly, check for other expected keys, like 'input_ids' and 'attention_mask'\n",
        "    if 'input_ids' in batch and 'attention_mask' in batch:\n",
        "        # Print the first row of input_ids to check for out-of-range token IDs\n",
        "        input_ids_first_row = batch['input_ids'][1]\n",
        "        print(f\"First row of 'input_ids': \\n{input_ids_first_row.tolist()}\")\n",
        "\n",
        "        # # Check if any token IDs are out of range\n",
        "        # vocab_size = tokenizer.vocab_size\n",
        "        # out_of_range_tokens = [token_id for token_id in input_ids_first_row if token_id >= vocab_size]\n",
        "        # if out_of_range_tokens:\n",
        "        #     print(f\"Out-of-range token IDs: {out_of_range_tokens}\")\n",
        "\n",
        "        # # Decode the first row of input_ids to text, if all token IDs are in range\n",
        "        # if not out_of_range_tokens:\n",
        "        #     decoded_inputs = tokenizer.decode(input_ids_first_row, skip_special_tokens=False)\n",
        "        #     print(f\"Decoded input tokens: {decoded_inputs}\")\n",
        "        # else:\n",
        "        #     print(\"Cannot decode input_ids due to out-of-range token IDs.\")\n",
        "\n",
        "        print(\"Text inputs are included in the DataLoader.\")\n",
        "        print(f\"Batch 'input_ids' shape: {batch['input_ids'].shape}\")\n",
        "        print(f\"Batch 'attention_mask' shape: {batch['attention_mask'].shape}\")\n",
        "\n",
        "        # # Decode the first row of input_ids to text\n",
        "        # decoded_inputs = tokenizer.decode(batch['input_ids'][0], skip_special_tokens=False)\n",
        "        # print(f\"Decoded input tokens: {decoded_inputs}\")\n",
        "\n",
        "        # Print the first row of labels, replacing ignore_index with the string '[IGNORE]'\n",
        "        labels = batch['labels'][1].tolist()\n",
        "        labels_str = ['[IGNORE]' if label == -100 else str(label) for label in labels]\n",
        "        print(f\"Labels: {labels_str}\")\n",
        "\n",
        "        # Print the first row of the attention_mask\n",
        "        attention_mask_str = batch['attention_mask'][1].tolist()\n",
        "        print(f\"Attention mask: {attention_mask_str}\")\n",
        "\n",
        "    # Optionally, display an image from the batch to visually confirm loading\n",
        "    if 'images' in batch:\n",
        "        image_tensor = batch['images'][1]\n",
        "        print(f\"First Row Image Data type: {image_tensor.dtype}\")\n",
        "        print(f\"First Row Image Shape: {image_tensor.shape}\")\n",
        "        print(f\"First Row Image Value range: [{image_tensor.min()}, {image_tensor.max()}]\")\n",
        "\n",
        "    break  # Only check the first batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "476a929f-b54a-4ff6-9b43-9cb6e25a5f21",
      "metadata": {
        "id": "476a929f-b54a-4ff6-9b43-9cb6e25a5f21",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# type(images_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e365070a-e9e2-4e5f-b8a3-c5319ed79af9",
      "metadata": {
        "id": "e365070a-e9e2-4e5f-b8a3-c5319ed79af9",
        "outputId": "fef2a8fe-4e6f-4ba1-a180-92bffb358adb",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-200\n"
          ]
        }
      ],
      "source": [
        "print(IMAGE_TOKEN_INDEX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a7b430-c4c1-4422-9427-6aac5691b752",
      "metadata": {
        "id": "e2a7b430-c4c1-4422-9427-6aac5691b752",
        "outputId": "ba529c4a-0382-4e92-8d9b-81c385859722",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<s> [INST]  \n",
            "What do you see in this picture? [/INST] A single black bishop </s><unk><unk><unk><unk>\n"
          ]
        }
      ],
      "source": [
        "# output_sample=[1, 733, 16289, 28793, 28705, 28705, 13, 3195, 511, 368, 1032, 297, 456, 5754, 28804, 733, 28748, 16289, 28793, 330, 2692, 2687, 287, 12019, 28705, 2, 0, 0, 0, 0]\n",
        "# print(tokenizer.decode(output_sample))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c172ee-4556-4735-b371-7ae6e6bbd66c",
      "metadata": {
        "id": "32c172ee-4556-4735-b371-7ae6e6bbd66c"
      },
      "outputs": [],
      "source": [
        "# test_query = \"[INST] <image>\\nWhat do you see in this picture? [/INST]\"\n",
        "\n",
        "# image_token_se = DEFAULT_IMAGE_TOKEN\n",
        "\n",
        "# print(f\"image_token_se is: {image_token_se}\")\n",
        "\n",
        "# print(tokenizer_image_token(test_query, tokenizer, IMAGE_TOKEN_INDEX))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b12b17-e327-4ef2-92bf-0ec319a9562b",
      "metadata": {
        "id": "76b12b17-e327-4ef2-92bf-0ec319a9562b"
      },
      "source": [
        "# Training Arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fd1937a-75ea-4808-b145-a74940a31c70",
      "metadata": {
        "id": "7fd1937a-75ea-4808-b145-a74940a31c70"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainerCallback, EarlyStoppingCallback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af2d811a-8e34-420f-9cc1-247b2642b4de",
      "metadata": {
        "id": "af2d811a-8e34-420f-9cc1-247b2642b4de",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # clear chache to empty memory\n",
        "\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b186861-4873-4f9f-89d5-be56a196497c",
      "metadata": {
        "id": "6b186861-4873-4f9f-89d5-be56a196497c"
      },
      "outputs": [],
      "source": [
        "# Define a custom callback\n",
        "class PrintBestCheckpointCallback(TrainerCallback):\n",
        "    def on_train_end(self, args, state, control, **kwargs):\n",
        "        # Print the path of the best checkpoint at the end of training\n",
        "        print(f\"Best model checkpoint: {state.best_model_checkpoint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef70d5c9-55a4-4029-9fb9-ff7daf949509",
      "metadata": {
        "id": "ef70d5c9-55a4-4029-9fb9-ff7daf949509",
        "tags": []
      },
      "outputs": [],
      "source": [
        "batch_size = 4  # Specify the batch size you want to use\n",
        "specific_directory = \"/home/jupyter/Models\"\n",
        "output_model_name = f\"{specific_directory}/{model_name}-latest-Vicuna\"\n",
        "\n",
        "# # Get the latest checkpoint if it exists\n",
        "# latest_checkpoint = get_latest_checkpoint(output_model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8d5921-cec9-43e4-be44-92059da94082",
      "metadata": {
        "id": "4b8d5921-cec9-43e4-be44-92059da94082"
      },
      "source": [
        "resume_from_checkpoint=True,  # Will automatically resume from the last checkpoint\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de5633fc-80f2-4bb7-8d4f-cb81fd644b7e",
      "metadata": {
        "id": "de5633fc-80f2-4bb7-8d4f-cb81fd644b7e",
        "outputId": "507a3d97-b433-47ca-9da8-fddc96a4abd5",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmdemo1\u001b[0m (\u001b[33mmsu_cvi_lab\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/jupyter/LLaVA/wandb/run-20240427_171807-w1i22k56</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/msu_cvi_lab/huggingface/runs/w1i22k56' target=\"_blank\">divine-frog-7</a></strong> to <a href='https://wandb.ai/msu_cvi_lab/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/msu_cvi_lab/huggingface' target=\"_blank\">https://wandb.ai/msu_cvi_lab/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/msu_cvi_lab/huggingface/runs/w1i22k56' target=\"_blank\">https://wandb.ai/msu_cvi_lab/huggingface/runs/w1i22k56</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6576' max='10950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 6576/10950 7:59:49 < 15:58:19, 0.08 it/s, Epoch 3/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4932</td>\n",
              "      <td>1.967800</td>\n",
              "      <td>1.984532</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5480</td>\n",
              "      <td>2.046900</td>\n",
              "      <td>1.953267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6028</td>\n",
              "      <td>1.897300</td>\n",
              "      <td>1.939575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6576</td>\n",
              "      <td>2.017900</td>\n",
              "      <td>1.932810</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n",
            "/opt/conda/envs/llava1.5env/lib/python3.8/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model checkpoint: /home/jupyter/Models/llava-v1.6-vicuna-7b-latest-Vicuna/checkpoint-4384\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6576, training_loss=0.6685205964131367, metrics={'train_runtime': 28807.8649, 'train_samples_per_second': 12.168, 'train_steps_per_second': 0.38, 'total_flos': 1.063026957579264e+18, 'train_loss': 0.6685205964131367, 'epoch': 3.0})"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# def compute_loss(model, inputs, return_outputs=False):\n",
        "#     labels = inputs.pop(\"labels\")\n",
        "#     outputs = model(**inputs)\n",
        "#     logits = outputs.logits\n",
        "#     loss_fct = CrossEntropyLoss(ignore_index=ignore_index)\n",
        "#     loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "#     return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_model_name,\n",
        "    learning_rate=1e-5,\n",
        "    # fp16=True, #for non ampere gpus\n",
        "    bf16=True,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=8,\n",
        "    dataloader_pin_memory=False,\n",
        "    save_total_limit=10,  # Limit the number of checkpoints to save\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=548,\n",
        "    eval_steps=548,\n",
        "    logging_steps=1,\n",
        "    num_train_epochs=5,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    label_names=[\"labels\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",  # validation loss this is the metric I am monitoring\n",
        "    greater_is_better=False,\n",
        "    report_to=None,\n",
        "    optim=\"adamw_torch\",\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={'use_reentrant':True}\n",
        "    # resume_from_checkpoint=latest_checkpoint,\n",
        "    # resume_from_checkpoint=True,  # Will automatically resume from the last checkpoint\n",
        ")\n",
        "\n",
        "# Initialize your custom callback\n",
        "print_best_checkpoint_callback = PrintBestCheckpointCallback()\n",
        "\n",
        "# Initialize early stopping callback\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience=4,  # Number of evaluation steps with no improvement after which training will be stopped\n",
        "    early_stopping_threshold=0.01  # Minimum change needed to qualify as an improvement\n",
        ")\n",
        "\n",
        "# Initialize the list of callbacks you want to use\n",
        "# Combine custom callback with the early stopping callback\n",
        "callbacks = [print_best_checkpoint_callback, early_stopping_callback]\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=eval_ds,\n",
        "    # compute_metrics=compute_metrics,\n",
        "    callbacks=callbacks,  # Add custom callback here\n",
        "    # compute_loss=compute_loss,  # Pass the custom compute_loss function\n",
        ")\n",
        "\n",
        "trainer.train('/home/jupyter/Models/llava-v1.6-vicuna-7b-latest-Vicuna/checkpoint-4384')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe6ed24-bb62-4959-a322-d804869b0be5",
      "metadata": {
        "id": "abe6ed24-bb62-4959-a322-d804869b0be5"
      },
      "source": [
        "##  Checking Trainer State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "98e4b7e5-ae7d-4faf-9dda-ebd650bb06e6",
      "metadata": {
        "id": "98e4b7e5-ae7d-4faf-9dda-ebd650bb06e6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Load the training history\n",
        "# with open('', 'r') as file:\n",
        "#     trainer_state = json.load(file)\n",
        "\n",
        "# # Extract loss values from the trainer_state\n",
        "# training_steps = []\n",
        "# training_losses = []\n",
        "# validation_steps = []\n",
        "# validation_losses = []\n",
        "\n",
        "# for log_entry in trainer_state['log_history']:\n",
        "#     if 'loss' in log_entry and 'step' in log_entry:  # This is a training loss entry\n",
        "#         training_steps.append(log_entry['step'])\n",
        "#         training_losses.append(log_entry['loss'])\n",
        "#     elif 'eval_loss' in log_entry and 'step' in log_entry:  # This is a validation loss entry\n",
        "#         validation_steps.append(log_entry['step'])\n",
        "#         validation_losses.append(log_entry['eval_loss'])\n",
        "\n",
        "# # Set up the plot\n",
        "# fig, ax = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
        "\n",
        "# # Plot training loss\n",
        "# ax[0].plot(training_steps, training_losses, label='Training Loss', color='blue')\n",
        "# ax[0].set_title('Training Loss')\n",
        "# ax[0].set_ylabel('Loss')\n",
        "# ax[0].grid(True)\n",
        "\n",
        "# # Plot validation loss\n",
        "# ax[1].plot(validation_steps, validation_losses, label='Validation Loss', color='orange')\n",
        "# ax[1].set_title('Validation Loss')\n",
        "# ax[1].set_ylabel('Loss')\n",
        "# ax[1].set_xlabel('Steps')\n",
        "# ax[1].grid(True)\n",
        "\n",
        "# # Define the tick values as multiples of 1095, considering the range of your steps\n",
        "# tick_step = 548\n",
        "# xticks = range(0, max(max(training_steps, default=0), max(validation_steps, default=0)) + tick_step, tick_step)\n",
        "# ax[1].set_xticks(xticks)  # Set x-ticks on both subplots\n",
        "\n",
        "# # Adjust layout\n",
        "# plt.tight_layout()\n",
        "\n",
        "# # Show the plot\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43f5626a-92dd-47ff-b087-2fd03b5f72b2",
      "metadata": {
        "id": "43f5626a-92dd-47ff-b087-2fd03b5f72b2"
      },
      "source": [
        "### Loading the checkpoint after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b65dfd90-f2d7-4b02-bad3-bf22446f73df",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "72c6c979b4674003b4ac55a0df72e440"
          ]
        },
        "id": "b65dfd90-f2d7-4b02-bad3-bf22446f73df",
        "outputId": "9e1eb49d-4ef6-4c67-cfdf-d97f66ace2a2",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72c6c979b4674003b4ac55a0df72e440",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vision_tower is loaded\n"
          ]
        }
      ],
      "source": [
        "# OPTIONALLY LOAD A DIFFERENT CHECKPOINT\n",
        "\n",
        "from peft import PeftModel\n",
        "\n",
        "adapter_path=\"/home/jupyter/Models/llava-v1.6-vicuna-7b-latest-Vicuna/checkpoint-4384\" #this is the path to the checkpoint to load\n",
        "\n",
        "## You have a choice now between either A or B:\n",
        "# A: Unload the current adapter (if this fails, just run option B to load the base model again).\n",
        "# model.unload()\n",
        "\n",
        "# B: Re-load the base model\n",
        "# the variables required to load the base model should already be declared at the top of this script.\n",
        "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "    model_path=model_path,\n",
        "    model_base=None,\n",
        "    model_name=model_name,\n",
        "    cache_dir='',\n",
        "    use_flash_attn=True,\n",
        "    # load_8bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
        "    # load_4bit=True #NOT SUPPORTED YET WITH THIS SCRIPT\n",
        ")\n",
        "\n",
        "# Now you can load the new model on top.\n",
        "model = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    adapter_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f285c6-3385-4398-a610-91b75a95df68",
      "metadata": {
        "id": "72f285c6-3385-4398-a610-91b75a95df68",
        "outputId": "ebc30215-4f3b-4eb4-84f1-bedf3c0aa37d",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PeftModel(\n",
            "  (base_model): LoraModel(\n",
            "    (model): LlavaLlamaForCausalLM(\n",
            "      (model): LlavaLlamaModel(\n",
            "        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x LlamaDecoderLayer(\n",
            "            (self_attn): LlamaSdpaAttention(\n",
            "              (q_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "              (rotary_emb): LlamaRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): LlamaMLP(\n",
            "              (gate_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=11008, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
            "              (down_proj): lora.Linear(\n",
            "                (base_layer): Linear(in_features=11008, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=11008, out_features=16, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): LlamaRMSNorm()\n",
            "            (post_attention_layernorm): LlamaRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): LlamaRMSNorm()\n",
            "        (vision_tower): CLIPVisionTower(\n",
            "          (vision_tower): CLIPVisionModel(\n",
            "            (vision_model): CLIPVisionTransformer(\n",
            "              (embeddings): CLIPVisionEmbeddings(\n",
            "                (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
            "                (position_embedding): Embedding(577, 1024)\n",
            "              )\n",
            "              (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "              (encoder): CLIPEncoder(\n",
            "                (layers): ModuleList(\n",
            "                  (0-23): 24 x CLIPEncoderLayer(\n",
            "                    (self_attn): CLIPAttention(\n",
            "                      (k_proj): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Dropout(p=0.05, inplace=False)\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                      )\n",
            "                      (v_proj): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Dropout(p=0.05, inplace=False)\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                      )\n",
            "                      (q_proj): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Dropout(p=0.05, inplace=False)\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                      )\n",
            "                      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "                    )\n",
            "                    (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                    (mlp): CLIPMLP(\n",
            "                      (activation_fn): QuickGELUActivation()\n",
            "                      (fc1): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Dropout(p=0.05, inplace=False)\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=1024, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=4096, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                      )\n",
            "                      (fc2): lora.Linear(\n",
            "                        (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "                        (lora_dropout): ModuleDict(\n",
            "                          (default): Dropout(p=0.05, inplace=False)\n",
            "                        )\n",
            "                        (lora_A): ModuleDict(\n",
            "                          (default): Linear(in_features=4096, out_features=16, bias=False)\n",
            "                        )\n",
            "                        (lora_B): ModuleDict(\n",
            "                          (default): Linear(in_features=16, out_features=1024, bias=False)\n",
            "                        )\n",
            "                        (lora_embedding_A): ParameterDict()\n",
            "                        (lora_embedding_B): ParameterDict()\n",
            "                      )\n",
            "                    )\n",
            "                    (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "                  )\n",
            "                )\n",
            "              )\n",
            "              (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (mm_projector): Sequential(\n",
            "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "          (1): GELU(approximate='none')\n",
            "          (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "463a50e5-16ce-4b0f-988b-1f97ba6a0798",
      "metadata": {
        "id": "463a50e5-16ce-4b0f-988b-1f97ba6a0798"
      },
      "source": [
        "## Eval after Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "638c6cb9-ae57-4a4b-8c29-152abb2259ed",
      "metadata": {
        "id": "638c6cb9-ae57-4a4b-8c29-152abb2259ed",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Temporarily disable the transformation to access the original data\n",
        "# eval_ds.reset_format()\n",
        "\n",
        "# # Limit the number of images to process\n",
        "# num_images_to_process = 5\n",
        "\n",
        "# # Iterate over each example in the evaluation dataset, but only up to num_images_to_process\n",
        "# for i in range(min(num_images_to_process, len(eval_ds))):\n",
        "#     # Access the original image and caption for the current row\n",
        "#     image = eval_ds[i]['image']\n",
        "#     caption = eval_ds[i]['caption']\n",
        "\n",
        "#     # Display the image using matplotlib\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')  # Turn off axis numbers and ticks\n",
        "#     plt.show()\n",
        "\n",
        "#     # Evaluate the model for the given image\n",
        "#     eval_model(\n",
        "#         tokenizer,\n",
        "#         model,\n",
        "#         image_processor,\n",
        "#         context_len,\n",
        "#         image,\n",
        "#         \"What do you see in this picture?\"\n",
        "#     )\n",
        "\n",
        "#     # Print the correct caption\n",
        "#     print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
        "\n",
        "# # Re-enable the transformation if needed\n",
        "# eval_ds.set_transform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ec919477-5dcf-43d7-91a0-2c4e88f63d56",
      "metadata": {
        "id": "ec919477-5dcf-43d7-91a0-2c4e88f63d56",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Temporarily disable the transformation to access the original data\n",
        "# eval_ds.reset_format()\n",
        "\n",
        "# # Limit the number of images to process\n",
        "# num_images_to_process = 5\n",
        "\n",
        "# # Iterate over each example in the evaluation dataset, but only up to num_images_to_process\n",
        "# for i in range(min(num_images_to_process, len(eval_ds))):\n",
        "#     # Access the original image and caption for the current row\n",
        "#     image = eval_ds[i]['image']\n",
        "#     caption = eval_ds[i]['caption']\n",
        "\n",
        "#     # Display the image using matplotlib\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')  # Turn off axis numbers and ticks\n",
        "#     plt.show()\n",
        "\n",
        "#     # Evaluate the model for the given image\n",
        "#     eval_model(\n",
        "#         tokenizer,\n",
        "#         model,\n",
        "#         image_processor,\n",
        "#         context_len,\n",
        "#         image,\n",
        "#         \"What do you see in this picture?\"\n",
        "#     )\n",
        "\n",
        "#     # Print the correct caption\n",
        "#     print(f\"\\nCorrect caption: {caption}\\n\\n\")\n",
        "\n",
        "# # Re-enable the transformation if needed\n",
        "# eval_ds.set_transform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9613b48-0c0d-4d6d-8c51-5de1424c8e1a",
      "metadata": {
        "id": "d9613b48-0c0d-4d6d-8c51-5de1424c8e1a"
      },
      "source": [
        "## Generating CSV from test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4924d63d-642b-45e8-acb5-2e0b75608a90",
      "metadata": {
        "id": "4924d63d-642b-45e8-acb5-2e0b75608a90"
      },
      "outputs": [],
      "source": [
        "# Example usage with a dataset\n",
        "ds_test = load_dataset(\"HoqueMahmudul/test_dataset\")\n",
        "\n",
        "test_ds = ds_test['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0abc6d-3ceb-4269-872f-2e0221c0a3f5",
      "metadata": {
        "id": "6b0abc6d-3ceb-4269-872f-2e0221c0a3f5",
        "outputId": "b9d8ba08-c5c7-46ec-dfd8-2b7b1a86223e",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('ImageCLEFmedical_Caption_2024_test_000004.jpg',\n",
              " '\"A glimpse into the intricate network of the human body: a clear view of the digestive system, highlighting the complex interplay of organs and the path of food through the gastrointestinal tract.\"')"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "captions[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aae96528-153d-4c46-84fe-076cb4c77a46",
      "metadata": {
        "id": "aae96528-153d-4c46-84fe-076cb4c77a46",
        "outputId": "ecf7e821-5648-4392-e9c3-ce2b43d59b74",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('ImageCLEFmedical_Caption_2024_test_000003.jpg',\n",
              " 'Postoperative esophagogram showing the esophagus and the gastric tube in the right thoracic cavity.')"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "captions[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "072d6b7c-0993-4e79-bbe7-bcd46273fd85",
      "metadata": {
        "id": "072d6b7c-0993-4e79-bbe7-bcd46273fd85"
      },
      "outputs": [],
      "source": [
        "# Accessing items one by one using test_ds[i] can be inefficient, especially for larger datasets.\n",
        "# If performance is an issue, consider batch processing if your model and infrastructure support it.\n",
        "\n",
        "\n",
        "# Now iterate through the test dataset to generate captions\n",
        "captions = []\n",
        "\n",
        "# Temporarily disable the transformation to access the original data\n",
        "test_ds.reset_format()\n",
        "\n",
        "for i in range(len(test_ds)):\n",
        "    # Access the original image and image name for the current row\n",
        "    image = test_ds[i]['image']\n",
        "    image_name = test_ds[i]['image_name']\n",
        "\n",
        "    # Evaluate the model for the given image to generate a predicted caption using eval_model function\n",
        "    predicted_caption = eval_model(\n",
        "        tokenizer,\n",
        "        model,\n",
        "        image_processor,\n",
        "        context_len,\n",
        "        image,\n",
        "        \"Can you write a caption for this image based on what you see in the image\"\n",
        "    )\n",
        "\n",
        "    # Append the correct and predicted captions as a tuple to the captions list\n",
        "    captions.append((image_name, predicted_caption))\n",
        "\n",
        "# Create a DataFrame from the captions list with the specified column titles\n",
        "df_captions_generation_test = pd.DataFrame(captions, columns=['ID', 'predicted_caption'])\n",
        "\n",
        "# Display the DataFrame to verify the contents\n",
        "print(df_captions_generation_test)\n",
        "\n",
        "# Optionally, save the DataFrame to a CSV file\n",
        "df_captions_generation_test.to_csv('/home/jupyter/Test dataset caption/llava-1.6-vicuna-4384-r32.csv', index=False)\n",
        "\n",
        "# Re-enable the transformation if needed\n",
        "test_ds.set_transform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e08dc1fd-29b1-4bc5-8d84-fb3865edecdf",
      "metadata": {
        "id": "e08dc1fd-29b1-4bc5-8d84-fb3865edecdf"
      },
      "source": [
        "# Finish"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de3333da-49eb-46c5-8af9-2f6dc08f0775",
      "metadata": {
        "id": "de3333da-49eb-46c5-8af9-2f6dc08f0775",
        "tags": []
      },
      "source": [
        "### generating csv over  eval set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb59bcf-8e7e-4829-a71d-90cd0c356e47",
      "metadata": {
        "id": "7fb59bcf-8e7e-4829-a71d-90cd0c356e47",
        "outputId": "d71913a8-f21d-4ddf-e432-a2d3a4a8516a",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                      correct_caption  \\\n",
            "0   Transabdominal ultrasonography of right lower ...   \n",
            "1   Ultrasonographic examination of an embryo vesi...   \n",
            "2   Chest x-ray demonstrating bilateral calcificat...   \n",
            "3   Transvaginal ultrasonography shows a septate c...   \n",
            "4                    Showing deployed Sapien 3 valve.   \n",
            "..                                                ...   \n",
            "95  Ultrasonography (sagittal view) showing an inc...   \n",
            "96   Intraoperative fluoroscopy showing positionin...   \n",
            "97  Computerized tomography scan of the brain in t...   \n",
            "98  Pulmonary blastomycosis chest X-ray image. Che...   \n",
            "99  X-ray image of aseptic loosening of the distal...   \n",
            "\n",
            "                                    predicted_caption  \n",
            "0   Transabdominal ultrasound showing the presence...  \n",
            "1   Transverse section of the fetal head at 32 wee...  \n",
            "2     Chest X-ray showing bilateral pleural effusion.  \n",
            "3   Transvaginal ultrasound image of the uterus an...  \n",
            "4   Chest radiograph of a patient with COVID-19 pn...  \n",
            "..                                                ...  \n",
            "95  Transvaginal ultrasound image of the uterus an...  \n",
            "96  Chest radiograph showing a large mediastinal m...  \n",
            "97  Axial CT scan showing a large left frontal mas...  \n",
            "98    Chest X-ray showing bilateral pleural effusion.  \n",
            "99  X-ray of the left knee showing the position of...  \n",
            "\n",
            "[100 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create an empty list to store both correct and predicted captions\n",
        "captions = []\n",
        "\n",
        "# Temporarily disable the transformation to access the original data\n",
        "eval_ds.reset_format()\n",
        "\n",
        "# # Example evaluation loop can use enumerate as well\n",
        "# for i, data in enumerate(eval_ds):\n",
        "#     # Prepare your data here\n",
        "#     # For example, for an image captioning task:\n",
        "#     image = data['image']\n",
        "#     true_caption = data['caption']\n",
        "\n",
        "# Iterate over each example in the evaluation dataset\n",
        "for i in range(len(eval_ds)):\n",
        "    # Access the original image and correct caption for the current row\n",
        "    image = eval_ds[i]['image']\n",
        "    correct_caption = eval_ds[i]['caption']\n",
        "\n",
        "    # Evaluate the model for the given image to generate a predicted caption using eval_model function\n",
        "    predicted_caption = eval_model(\n",
        "        tokenizer,\n",
        "        model,\n",
        "        image_processor,\n",
        "        context_len,\n",
        "        image,\n",
        "        \"What do you see in this picture?\"\n",
        "    )\n",
        "\n",
        "    # Append the correct and predicted captions as a tuple to the captions list\n",
        "    captions.append((correct_caption, predicted_caption))\n",
        "\n",
        "# Create a DataFrame from the captions list with the specified column titles\n",
        "df_captions = pd.DataFrame(captions, columns=['correct_caption', 'predicted_caption'])\n",
        "\n",
        "# Display the DataFrame to verify the contents\n",
        "print(df_captions)\n",
        "\n",
        "# Optionally, save the DataFrame to a CSV file\n",
        "df_captions.to_csv('/home/jupyter/captions_comparison.csv', index=False)\n",
        "\n",
        "# Re-enable the transformation if needed\n",
        "eval_ds.set_transform(lambda batch: tokenize_and_create_labels(batch, image_processor, tokenizer, model, device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63b887d5-13f7-45dd-ba18-1f07ffc2a144",
      "metadata": {
        "id": "63b887d5-13f7-45dd-ba18-1f07ffc2a144",
        "outputId": "a3dc82b3-34a6-472f-d206-49288d585a62",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available fields in the first item of eval_ds: dict_keys(['image', 'caption'])\n"
          ]
        }
      ],
      "source": [
        "# Print the keys of the first item in the dataset\n",
        "if len(eval_ds) > 0:\n",
        "    first_item_keys = eval_ds[0].keys()\n",
        "    print(\"Available fields in the first item of eval_ds:\", first_item_keys)\n",
        "else:\n",
        "    print(\"The dataset is empty.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "844638ff-cdae-456b-9c4c-07de49abe13a",
      "metadata": {
        "id": "844638ff-cdae-456b-9c4c-07de49abe13a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# # Print the first 3 items in the dataset\n",
        "# for i in range(min(3, len(eval_ds))):\n",
        "#     print(f\"Item {i}:\")\n",
        "#     for key in eval_ds[i].keys():\n",
        "#         value = eval_ds[i][key]\n",
        "#         # Since some values can be very large (like tensors), we print their type and size instead of the values themselves\n",
        "#         if hasattr(value, 'shape'):  # Check if the value is a tensor or similar\n",
        "#             print(f\"  {key}: {type(value)} with shape {value.shape}\")\n",
        "#         else:\n",
        "#             print(f\"  {key}: {value}\")\n",
        "#     print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ce7a27-8255-4898-9fb2-d818a3314c6e",
      "metadata": {
        "id": "92ce7a27-8255-4898-9fb2-d818a3314c6e"
      },
      "outputs": [],
      "source": [
        "# save the DataFrame to a CSV file\n",
        "df_predictions.to_csv('/home/jupyter/predicted_captions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ca1359-230b-4487-9998-9d6f73dfb605",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "20dbbc975fdb4fdb8e2897fd2a945c63"
          ]
        },
        "id": "41ca1359-230b-4487-9998-9d6f73dfb605",
        "outputId": "47c08fc6-1762-485d-a432-07d9078dacfa",
        "tags": []
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20dbbc975fdb4fdb8e2897fd2a945c63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Required when quantizing models/data that are gated on HuggingFace, and required for pushing models to HuggingFace.\n",
        "!pip install huggingface_hub -q -U\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae65972-080e-4c4b-8a8e-8e0ebc5157ea",
      "metadata": {
        "id": "8ae65972-080e-4c4b-8a8e-8e0ebc5157ea",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model=model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c106e47b-0adb-480c-a1f8-05e375391d1a",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3de191738f3a4cceba8fc2050b421a69",
            "119212b46a7243f0919aa3f39f25974a"
          ]
        },
        "id": "c106e47b-0adb-480c-a1f8-05e375391d1a",
        "outputId": "f62c68dd-84a6-4e6c-bede-aabab6d87b4a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# tokenizer.push_to_hub(f\"HoqueMahmudul/{output_model_name}\", private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0154bef4-5dec-449a-95e9-9b88a807833b",
      "metadata": {
        "id": "0154bef4-5dec-449a-95e9-9b88a807833b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "environment": {
      "kernel": "conda-env-llava1.5env-llava1.5env",
      "name": "workbench-notebooks.m118",
      "type": "gcloud",
      "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
